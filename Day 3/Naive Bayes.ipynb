{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fc6ece4",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier\n",
    "\n",
    "In this tutorial, you will learn how to classify the email as spam or not using the Naive Bayes Classifier.\n",
    "\n",
    "<img src=\"img/spam-filter.png\">\n",
    "\n",
    "First, let's review a little bit the characteristics of Naive Bayes Classifiers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b679bc47",
   "metadata": {},
   "source": [
    "### Pro and cons of Naive Bayes Classifiers\n",
    "Pros:\n",
    "* Computationally fast\n",
    "* Simple to implement\n",
    "* Works well with small datasets\n",
    "* Works well with high dimensions\n",
    "* Perform well even if the Naive Assumption is not perfectly met. In many cases, the approximation is enough to build a good classifier.\n",
    "\n",
    "Cons:\n",
    "* Require to remove correlated features because they are voted twice in the model and it can lead to over inflating importance.\n",
    "* If a categorical variable has a category in test data set which was not observed in training data set, then the model will assign a zero probability. It will not be able to make a prediction. This is often known as “Zero Frequency”. To solve this, we can use the smoothing technique. One of the simplest smoothing techniques is called [Laplace estimation](https://stats.stackexchange.com/questions/108797/in-naive-bayes-why-bother-with-laplace-smoothing-when-we-have-unknown-words-in). `sklearn` applies Laplace smoothing by default when you train a Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8883b71d",
   "metadata": {},
   "source": [
    "### Popular use cases for Naive Bayes models\n",
    "\n",
    "* Spam Detection\n",
    "* Classification of the customer\n",
    "* Loan Classification\n",
    "* Health Risk Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d9b205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29695792",
   "metadata": {},
   "source": [
    "We will load the dataset containing the spam information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf98ab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.loadtxt(\"data/spambase.data\",delimiter=\",\")\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fd259b",
   "metadata": {},
   "source": [
    "Print the number of rows and attributes contained in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64c427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e2151c",
   "metadata": {},
   "source": [
    "If you check the file `data/spambase_doc.txt` you can read the relevant information of the dataset.\n",
    "\n",
    "For our purposes, we will use the following attributes:\n",
    "* The first 48 attributes as training features. These are continuous values ranged from 0 to 100, representing the percentage of words in the e-mail that match 48 different keywords, usually associated with spam/not spam e-mails.\n",
    "* The last attribute as target. It is a binary class attribute representing if the e-mail was considered spam (1) or not (0).\n",
    "\n",
    "Extract the features and target matrices by using `numpy` slicing operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0505f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ...\n",
    "y = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5055f764",
   "metadata": {},
   "source": [
    "Split the dataset into train and test with the test size of 0.33, and _ramdom_state=17_. For this, use `sklearn`'s method `train_test_split`, whose documentation can be found in https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63ea70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e42f38-c647-485e-913d-7d43b6ee927e",
   "metadata": {},
   "source": [
    "Model a [Gaussian Naive Bayes Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) on the dataset. For this, you can use the `GaussianNB` implementation by `sklearn`.\n",
    "\n",
    "After this, train the model using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1016e0fb-384f-469f-a5e5-c77088963293",
   "metadata": {},
   "outputs": [],
   "source": [
    "GaussNB = ...\n",
    "... # fit your model\n",
    "\n",
    "print(GaussNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd047c13",
   "metadata": {},
   "source": [
    "Using the train model, make predictions for the test features. Recall that you can use the function `.predict()` for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16bbb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = ...\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65e7782",
   "metadata": {},
   "source": [
    "Use the `accuracy_score` function to compare the predictions against the real target values (`y_test`). Print the accuracy obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421d851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = ...\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfb6a95",
   "metadata": {},
   "source": [
    "### Compare with Logistic regression\n",
    "\n",
    "Let's make a quick comparison with a Logistic regression model trained on the same data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3334e574",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(x_train, y_train)\n",
    "print(log_reg)\n",
    "y_predict = log_reg.predict(x_test)\n",
    "acc = accuracy_score(y_test, y_predict)\n",
    "print (acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4f6a44",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Well done! We have implemented a spam detector algorithm that detects spam in emails with ~0.81 accuracy. A couple of remarks:\n",
    "1. Since we have chosen the best model based on the test results, in reality this dataset has acted as **validation** one. In order to perform a strict evaluation of the quality of the model, we should test the chosen algorithm against a fresh batch of data.\n",
    "2. We have chosen the accuracy score as performance metric. However, depending on the problem statement, other metrics would be more appropriate. Some of them are the Precision, Recall, F-1 Score, or Area Under the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbc227f-1636-4aa7-9042-693412a0db14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Linear Regression\n",
    "\n",
    "In this tutorial we will perform a simple linear regression. We will implement a solution both in scratch, using the concepts learned in class, and then we will make use of `scikit-learn`, a library that provides simple and efficient tools for data mining and data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have an error about missing modules, try installing them in the command line:\n",
    "```\n",
    "conda install pandas\n",
    "conda install numpy\n",
    "conda install matplotlib\n",
    "conda install sklearn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating (\"Learning\") Model Coefficients\n",
    "\n",
    "Generally speaking, coefficients are estimated using the **least squares criterion**, which means we find the line (mathematically) which minimizes the **sum of squared residuals** (or \"sum of squared errors\"):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/estimating_coefficients.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What elements are present in the diagram?\n",
    "- The black dots are the **observed values** of x and y.\n",
    "- The blue line is our **least squares line**.\n",
    "- The red lines are the **residuals**, which are the distances between the observed values and the least squares line.\n",
    "\n",
    "How do the model coefficients relate to the least squares line?\n",
    "- $\\beta_0$ is the **intercept** (the value of $y$ when $x$=0)\n",
    "- $\\beta_1$ is the **slope** (the change in $y$ divided by change in $x$)\n",
    "\n",
    "Here is a graphical depiction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/slope_intercept.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For starters, it is useful to understand how to implement the model from scratch. Knowing how the packages work behind the scenes is important so you are not just blindly implementing the models.\n",
    "\n",
    "To get started, let’s simulate some data and look at how the predicted values ($\\hat{y}$) differ from the actual value ($y$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 'random' data\n",
    "np.random.seed(0)\n",
    "X = 2.5 * np.random.randn(100) + 1.5   # Array of 100 values with mean = 1.5, stddev = 2.5\n",
    "res = 0.5 * np.random.randn(100)       # Generate 100 residual terms\n",
    "y = 2 + 0.3 * X + res                  # Actual values of Y\n",
    "\n",
    "# Create pandas dataframe to store our X and y values\n",
    "df = pd.DataFrame(\n",
    "    {'X': X,\n",
    "     'y': y}\n",
    ")\n",
    "\n",
    "# Show the first five rows of our dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df.X,df.y,'bo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of the Ordinary Least Squares (OLS) method is to find values of $\\beta_0$ (intercept) and $\\beta_1$ (slope) that minimize the sum of the squared difference between $y$ and $\\hat{y}$. We will not go through the derivation here, but using calculus we can show that the values of the unknown parameters are as follows:\n",
    "\n",
    "$$\\beta_1 = \\frac {\\sum_{i=1}^{n} (X_i - \\overline{X})(y_i - \\overline{y})}{\\sum_{i=1}^{n} (X_i - \\overline{X})^2}$$\n",
    "\n",
    "$$\\beta_0 = \\hat{y} - \\beta_1 * \\overline{X}$$\n",
    "\n",
    "Therefore, to estimate $y$ using the OLS method, we need to calculate `xmean` and `ymean`, the covariance of $X$ and $y$ (`xycov`), and the variance of $X$ (`xvar`) before we can determine the values for the intercept and the slope.\n",
    "\n",
    "**Task**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of X and y\n",
    "xmean = ...\n",
    "ymean = ...\n",
    "\n",
    "# Calculate the terms needed for the numerator and denominator of beta_1\n",
    "df['xycov'] = ...\n",
    "df['xvar'] = ...\n",
    "\n",
    "# Calculate beta_0 and beta_1\n",
    "slope = ...\n",
    "intercept = ...\n",
    "print(f'intercept = {intercept}')\n",
    "print(f'slope = {slope}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we now have an estimate for $y$! Our model can be written as $\\hat{y} = 2.003 + 0.323 X$, and we can make predictions.\n",
    "\n",
    "**Task**: Make the predictions for the matrix $X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = ...\n",
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s plot our prediction `ypred` against the actual values of `y`, to get a better visual understanding of our model.\n",
    "\n",
    "**Remark**: do not worry if you do not understand the code to plot the data. In tomorrow's extra material you will find an introduction to visualization with `matplotlib`, `seaborn` and `folium`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot regression against actual data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(X, ypred)     # regression line\n",
    "plt.plot(X, y, 'ro')   # scatter plot showing actual data\n",
    "plt.title('Actual vs Predicted')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue line is our line of best fit, $\\hat{y} = 2.003 + 0.323 X$. We can see from this graph that there is a positive linear relationship between $X$ and $y$. Using our model, we can predict $y$ from any values of $X$!\n",
    "\n",
    "**Task**: Predict the result for x = 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 10\n",
    "pred = ...\n",
    "\n",
    "print(f'x = {x} --> y = {pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen how to implement a simple linear regression from scratch, we will use `scikit-learn`, a library that has implementations of this and other Machine Learning algorithms.\n",
    "\n",
    "Since the algorithms implemented by the library assume multiple variables, we have to reshape our input variable $X$ from an array to a matrix of _len(X)_ rows and 1 column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2 = X.reshape(-1, 1)\n",
    "X.shape, X_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scikit-learn** provides an easy way to train the model, via the `LinearRegression` interface.\n",
    "\n",
    "**Task**: Create a linear regression model and train it (check the function `.fit()`) using X and y.\n",
    "\n",
    "Check https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html for detailed information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit model\n",
    "lm = ...\n",
    "model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no need to calculate the values for the intercept and the slope ourselves. Find out how to find these values by looking at the model attributes (_Hint:_ check https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'intercept = {...}')\n",
    "print(f'slope = {...}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit-learn` also allows to easily predict values by using `.predict()`. \n",
    "\n",
    "**Task**: Make the predictions for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = ...\n",
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again visualize the output of our model's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot regression against actual data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(X, ypred)     # regression line\n",
    "plt.plot(X, y, 'ro')   # scatter plot showing actual data\n",
    "plt.title('Actual vs Predicted')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Let's make a prediction for x=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[10]]\n",
    "pred = ...\n",
    "\n",
    "print(f'x = {x} --> y = {pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Well done!**\n",
    "\n",
    "In following notebooks we will implement Linear Regression models with multiple variables, as well as Logistic Regression and more advanced Machine Learning algorithms. Keep it up!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

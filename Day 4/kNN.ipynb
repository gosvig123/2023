{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Credits: Applied Data Analysis (ADA) course at EPFL (https://dlab.epfl.ch/teaching/fall2020/cs401/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview on K-NN\n",
    "\n",
    "Let's create some complex shapes to observe how K-Nearest Neighbors (K-NN) behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_gaussian_quantiles\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "n_samples_moons = 500\n",
    "n_samples_circles = 100\n",
    "X_moons, y_moons = make_moons(n_samples=n_samples_moons, noise=0.2, random_state=0)\n",
    "X_circles, y_circles = make_gaussian_quantiles(n_samples=n_samples_circles, random_state=0)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(9,4))\n",
    "\n",
    "axs[0].scatter(X_moons[:,0], X_moons[:,1], c=y_moons)\n",
    "axs[0].set_title('Moon Data')\n",
    "\n",
    "axs[1].scatter(X_circles[:,0], X_circles[:,1], c=y_circles)\n",
    "axs[1].set_title('Circles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting function to predict the class of different areas of the features space. Do not worry about the inners of this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting helper \n",
    "# Source: https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_prediction(model, X, y, ax, K):\n",
    "    # step size in the mesh\n",
    "    h = .02\n",
    "    # Create color maps\n",
    "    cmap_light = ListedColormap(['orange', 'cyan', 'cornflowerblue'])\n",
    "    cmap_bold = ListedColormap(['darkorange', 'c', 'darkblue'])\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='auto')\n",
    "\n",
    "    # Plot also the training points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n",
    "                edgecolor='k', s=20)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_title(\"K = {}\".format(K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create K-NN models for both dataset with *K=1*, *K=15*, *K=len(dataset)*. For this purpose, you can use `sklearn`'s implementation `kNeighborsClassifier`, which was already imported above. Check https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K = 1\n",
    "clf_moons_1 = ...\n",
    "# fit clf_moons_1\n",
    "clf_circles_1 = ...\n",
    "# fit clf_circles_1\n",
    "\n",
    "# K = 15\n",
    "clf_moons_15 = ...\n",
    "# fit clf_moons_15\n",
    "clf_circles_15 = ...\n",
    "# fit clf_circles_15\n",
    "\n",
    "# K = all\n",
    "clf_moons_all = ...\n",
    "# fit clf_moons_all\n",
    "clf_circles_all = ...\n",
    "# fit clf_circles_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before visualizing the results, what do you think will happen to each of the models created? How do you think the selectiong for `K` will influence the classification results? What would happen if *K=1*? And if *K=len(dataset)*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`insert your thoughts here`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(17,9))\n",
    "\n",
    "plot_prediction(clf_moons_1, X_moons, y_moons, axs[0][0], 1)\n",
    "plot_prediction(clf_moons_15, X_moons, y_moons, axs[0][1], 15)\n",
    "plot_prediction(clf_moons_all, X_moons, y_moons, axs[0][2], n_samples_moons)\n",
    "\n",
    "\n",
    "plot_prediction(clf_circles_1, X_circles, y_circles, axs[1][0], 1)\n",
    "plot_prediction(clf_circles_15, X_circles, y_circles, axs[1][1], 15)\n",
    "plot_prediction(clf_circles_all, X_circles, y_circles, axs[1][2], n_samples_circles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation below:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Choosing an appropriate hyperparameter `K` is key to the success of modelling a classification task using K-NN. As general rules: \n",
    "* Small K --> low bias, high variance. The extreme case of this is *K=1*, where the bias will be minimal but the modelling is probably overfitting the training data. Observe the moons dataset, where the outliers are creating a classification area which is clearly not correct. If we check the circles dataset, we can also see the sharpness of the classification boundaries.\n",
    "* Large K --> high bias, low variance. Averaging between more datapoints will smooth the classification boundary, but this might lead to an underfitting of the training data. Check the extreme case *K=len(dataset)*, where the class selected for any point in the space is the one with the most number of samples.\n",
    "* Choice of K depends on the dataset. As you may observe, choosing *K=15* seems to be suitable for our moons dataset, but it is suboptimal for the circles dataset (a smaller K would probably perform better).\n",
    "\n",
    "In order to choose the appropriate hyperparameter `K`, the training data must be subdivided into training a validation, and the validation subset would be used to evaluate what is the best choice for `K` given the models created with the training subset. After this, the final evaluation of the model must be done using the testing data, in order to avoid potential biases of the hypermarameters choice.\n",
    "\n",
    "**We recommend that you try out different K values to observe how the predictions change! At the same time, try changing the number of samples for each dataset, so that you can see how this also affects the search for an optimal K**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Comparison against a Logistic Regressor\n",
    "\n",
    "We will train a Logistic Regression model on the data and explore the decision boundary learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_moons, y_moons)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "plot_prediction(log_reg, X_moons, y_moons, ax, \"NaN (Logistic Regression)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

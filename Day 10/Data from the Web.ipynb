{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data from the Web\n",
    "\n",
    "#### What do you find in this Notebook?\n",
    "\n",
    "The purpose of this tutorial session is to offer a **quick** overview on how to scrape a Web page. In details, we illustrate the two main libraries used for this purpose: [Requests](https://requests.kennethreitz.org/en/master/) and [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/). Afterwords, we show how to retrieve data from the Web.\n",
    "\n",
    "---\n",
    "\n",
    "*Credits to: [Tiziano Piccardi](https://github.com/tizianopiccardi)*\n",
    "\n",
    "*Updated by: [Ekaterina Svikhnushina](https://github.com/Sea94) and [Pablo Cañas](https://github.com/pcanas)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is web scraping\n",
    "\n",
    "The web is a massive knowledge base. Most of the data available is not structured and it is difficult to generate a dataset from which you can extract valuable insights. \n",
    "\n",
    "Web scraping is the process of using bots to extract content and data from a website. With web scraping, you can extract underlying HTML code and data stored in a database. As a result, you can generate your own datasets for your data science projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTP and HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access to websites is conducted via the [HTTP](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol) protocol. In such, a request to obtain the content of a website (GET request) is sent to a server, and the server sends back HTML code with the content of the website, which renders in our screens. We can also send information to the server via POST requests, for example, when filling a form.\n",
    "\n",
    "In the following extract, we show the basic HTML structure of a website. The `head` section contains the title and metadata, while the `body` section contains the content of the website.\n",
    "```\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Page Title</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>This is a Heading</h1>\n",
    "        <p>Hello world!</p>\n",
    "    </body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remark: JSON\n",
    "\n",
    "[JSON](https://en.wikipedia.org/wiki/JSON) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute–value pairs and arrays (or other serializable values). It is a very common data format, with a diverse range of applications, one example being web applications that communicate with a server.\n",
    "\n",
    "Example of a basic JSON format:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"firstName\": \"John\",\n",
    "  \"lastName\": \"Smith\",\n",
    "  \"isAlive\": true,\n",
    "  \"age\": 27,\n",
    "  \"address\": {\n",
    "    \"streetAddress\": \"21 2nd Street\",\n",
    "    \"city\": \"New York\",\n",
    "    \"state\": \"NY\",\n",
    "    \"postalCode\": \"10021-3100\"\n",
    "  },\n",
    "  \"phoneNumbers\": [\n",
    "    {\n",
    "      \"type\": \"home\",\n",
    "      \"number\": \"212 555-1234\"\n",
    "    },\n",
    "    {\n",
    "      \"type\": \"office\",\n",
    "      \"number\": \"646 555-4567\"\n",
    "    }\n",
    "  ],\n",
    "  \"children\": [],\n",
    "  \"spouse\": null\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fetch data from the Web with Python, you need to get use to two essential libraries:\n",
    "\n",
    " * [`Requests (HTTP)`](https://requests.kennethreitz.org/en/master/): get the `html` page to parse.\n",
    "\n",
    " * [`Beautiful Soup (HTML Parsing)`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/): parse the `html` and extract data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a `get` request\n",
    "\n",
    "The [GET method](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol#Request_methods) retrieves information from the server.\n",
    "\n",
    "We start scraping this website: https://httpbin.org/ - HTTP Request & Response Service. The website offers some useful endpoints [1] to check the content of our request. Some of them provide an 'echo service' that reply with the request received.\n",
    "\n",
    "[1] Endpoint is a web address (URL) at which clients of a specific service can gain access to it. By referencing that URL, clients can get to operations provided by that service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: Get request basics\n",
    "Here we show an example on how use a get request. In particular, you see that we can get different information about the response:\n",
    "\n",
    "* The status code [2] which tells us whether everything is fine and if the request worked\n",
    "* The headers\n",
    "* Body of the response (typically HTML for webpages or JSON/XML for web services)\n",
    "\n",
    "[2] Find the reminder of HTTP status codes [here](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes). Some typical codes are: **200 OK** (standard response for successful HTTP requests) and **404 Not Found** (the requested resource could not be found but may be available in the future)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** this is an echo service, what you see is what we sent to the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the request\n",
    "r = requests.get('https://httpbin.org/ip') # /ip: Returns the requester's IP Address.\n",
    "\n",
    "print('Response status code: {0}\\n'.format(r.status_code))\n",
    "print('Response headers: {0}\\n'.format(r.headers))\n",
    "print('Response body: {0}'.format(r.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: Parsing JSON string from the response\n",
    "\n",
    "If the body of the response is a JSON string, Requests offers a convenient way to parse the text and get a Python dictionary.\n",
    "\n",
    "Let's try to get the current time from here: http://worldtimeapi.org/api/timezone/Europe/Madrid – a simple web service that returns the local-time for a given timezone as either JSON (by default) or plain-text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('http://worldtimeapi.org/api/timezone/Europe/Madrid')\n",
    "\n",
    "print('Response body (parsed json):')\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3: Including parameters into get request\n",
    "\n",
    "This time, the `url` has been slightly changed to include a parameter (key1).\n",
    "\n",
    "Remember that the with the GET method the parameters are part of the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://httpbin.org/get?key1=value1')\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a `post` request\n",
    "The [POST method](https://en.wikipedia.org/wiki/POST_(HTTP)) requests that a web server accepts the data enclosed in the body of the request message, most likely for storing it.\n",
    "\n",
    "A POST request can have the paramenters in the body. Let's how to do this with Requests library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'key1': 'value1', 'key2': 'value2'}\n",
    "r = requests.post('https://httpbin.org/post', data=payload)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a request and extract the Page Title!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Send the request and get the `html`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the request\n",
    "r = requests.get('https://httpbin.org/html')\n",
    "r.text[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Thus, we start to use our beloved `BeautifulSoup` to parse the HTML and we get the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the header\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "soup.h1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get interesting data - DBLP\n",
    "\n",
    "*DBLP is a computer science bibliography website. Starting in 1993 at the University of Trier, Germany, it grew from a small collection of HTML files and became an organization hosting a database and logic programming bibliography site. DBLP listed more than 3.66 million journal articles, conference papers, and other publications on computer science in July 2016, up from about 14,000 in 1995.*\n",
    "\n",
    "<div align=\"right\">https://en.wikipedia.org/wiki/DBLP</div> \n",
    "\n",
    "We want to check the distribution of the publications by year of the president of our Prof. Vicenç Soler \n",
    "\n",
    "First of all, let's check the page with the data we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://dblp.org/pid/20/4669.html'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The page is public and accessible with a browser using a simple GET:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(URL)\n",
    "page_body = r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the page content is downloaded and we can inspect the body of the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is pure HTML, and we need BeautifulSoup to parse the content. Let's check the documentation of this library! https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page_body, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the page is parsed and we can read the data we need!\n",
    "\n",
    "For example, let's get the title! Are we in the right page?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! And we can get the clean text without HTML tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more complex query now! Let's find all the links in the page. \n",
    "\n",
    "Hint: HTML a link is defined using the tag &lt;a&gt;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the first the link:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to get all the links pointing to an external website (not DBLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on. Now we want to extract the sections that contain the publication details.\n",
    "\n",
    "**The easiest way is to inspect the DOM of the web page with a browser.** Check with your browser how to isolate the portions of the page that represent publications. --- Use Google Chrome Inspector ---\n",
    "\n",
    "Ok, each row is composed by a &lt;li&gt; tag and has a class called 'entry'. Can we get these publications?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the number of rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the titles of the publications!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a script that gathers all Vicenç's publications and stores them in a DataFrame with the columns `title` (string), `authors` (list) and `year` (integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a bar plot that shows the number of publications that Vicenç has carried out each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

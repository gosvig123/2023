{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c57c6412",
   "metadata": {},
   "source": [
    "*Credits: Applied Data Analysis (ADA) course at EPFL (https://dlab.epfl.ch/teaching/fall2020/cs401/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b901de1d",
   "metadata": {},
   "source": [
    "## Applied Machine Learning\n",
    "\n",
    "Welcome to our last tutorial of the course, congratulations for making it until here! In this tutorial, we will go through the main concepts learned during the course using a real-world use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30df4cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from itertools import combinations \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3d7c2e",
   "metadata": {},
   "source": [
    "### All you need is love… And a pet!\n",
    "\n",
    "<img src=\"img/dataset-cover.jpg\" width=\"920\">\n",
    "\n",
    "Here we are going to build a classifier to predict whether an animal from an animal shelter will be adopted or not (aac_intakes_outcomes.csv, available at: https://www.kaggle.com/aaronschlegel/austin-animal-center-shelter-intakes-and-outcomes/version/1#aac_intakes_outcomes.csv). You will be working with the following features:\n",
    "\n",
    "1. *animal_type:* Type of animal. May be one of 'cat', 'dog', 'bird', etc.\n",
    "2. *intake_year:* Year of intake\n",
    "3. *intake_condition:* The intake condition of the animal. Can be one of 'normal', 'injured', 'sick', etc.\n",
    "4. *intake_number:* The intake number denoting the number of occurrences the animal has been brought into the shelter. Values higher than 1 indicate the animal has been taken into the shelter on more than one occasion.\n",
    "5. *intake_type:* The type of intake, for example, 'stray', 'owner surrender', etc.\n",
    "6. *sex_upon_intake:* The gender of the animal and if it has been spayed or neutered at the time of intake\n",
    "7. *age_upon\\_intake_(years):* The age of the animal upon intake represented in years\n",
    "8. *time_in_shelter_days:* Numeric value denoting the number of days the animal remained at the shelter from intake to outcome.\n",
    "9. *sex_upon_outcome:* The gender of the animal and if it has been spayed or neutered at time of outcome\n",
    "10. *age_upon\\_outcome_(years):* The age of the animal upon outcome represented in years\n",
    "11. *outcome_type:* The outcome type. Can be one of ‘adopted’, ‘transferred’, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f558b0c",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31867e77",
   "metadata": {},
   "source": [
    "First things first! Let's load the data into memory using Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61572146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n",
    "original_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2544ce",
   "metadata": {},
   "source": [
    "Let's check if there are any missing values in the DataFrame. [This website](https://datatofish.com/check-nan-pandas-dataframe/) gives a great overview on the possibilities that you have to check this. Try to print how many values of each column are missing. `isna`. `isnull`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb32c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48bb9ea",
   "metadata": {},
   "source": [
    "Since the number of missing values is very small compared to the data size, and since most of the missing values correspond to the target variable `outcome_type`, we have decided to just drop the instances where there exists any null value. *Hint*: to do this, you may want to use pandas' `dropna`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681394fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The length of the data with all rows is : {}'.format(len(original_data)))\n",
    "original_data = ...\n",
    "print('The length of the data without the rows with nan value is: {}'.format(len(original_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae61d7c",
   "metadata": {},
   "source": [
    "How many different values does the column _outcome\\_type_ have? Print them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68004144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033e397f",
   "metadata": {},
   "source": [
    "In this task, we will just focus on whether the animal was adopted or not. Create the column _adopted_, that will have a value 1 if the value for _outcome\\_type_ is 'Adoption', and 0 otherwise. `apply`, `lambda`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1403ebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = original_data.copy()\n",
    "data['adopted'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2067acea",
   "metadata": {},
   "source": [
    "Now, drop the column _outcome\\_type_, since we do not need it anymore. `drop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc14b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ...\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4bdef3",
   "metadata": {},
   "source": [
    "Select the data features (all but _adopted_) and the data label (_adopted_) for the task. After this, split the data into a training set (80%) and a test set (20%). You may use sklearn's function `train_test_split`. Use a random_state=42. You can further check the documentation in: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9d8a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = data.drop(columns=['adopted'])\n",
    "data_label = data['adopted']\n",
    "\n",
    "train_features, test_features, train_label, test_label = ...\n",
    "\n",
    "print('Length of the train dataset : {}'.format(len(train_features)))\n",
    "print('Length of the test dataset : {}'.format(len(test_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdd99cc",
   "metadata": {},
   "source": [
    "The dataset contains categorial features. We need to convert this to a suitable numerical representation. We will use pandas' `get_dummies` function to use a dummy-variable encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e0c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_categorical = ...\n",
    "train_categorical.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6214fae",
   "metadata": {},
   "source": [
    "We will do the same with the test set. However, we have to take into account that the features in the test set must be matched with the ones in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6f46db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we use only the features available in the test set\n",
    "test_categorical = ...\n",
    "test_categorical.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dd3595",
   "metadata": {},
   "source": [
    "Let's normalize the values of each feature in the data to have mean 0 and variance 1. For this, we will use sklearn's `StandardScaler` function. Check out more in its documentation https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "\n",
    "Declare an instance of the scaler and fit it with the training features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fc70fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = ...\n",
    "# fit the scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6375897c",
   "metadata": {},
   "source": [
    "Now, normalize the training features. *Hint:* use `.transform()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358763a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features = ...\n",
    "\n",
    "# The output of the .transform() function is a numpy matrix. We transform it back to a DataFrame\n",
    "train_features_std = pd.DataFrame(scaled_features, index=train_categorical.index, columns=train_categorical.columns)\n",
    "train_features_std.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d16e05c",
   "metadata": {},
   "source": [
    "We will also normalize the test features with the same scaler (mean and variance are extracted from the training columns). We do this because we assume that the training data is representative enough of our sample, and we should not look at the distribution of the test set and instead assume that it will be similar to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b929e14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features = ...\n",
    "\n",
    "# The output of the .transform() function is a numpy matrix. We transform it back to a DataFrame\n",
    "test_features_std = pd.DataFrame(scaled_features, index=test_categorical.index, columns=test_categorical.columns)\n",
    "test_features_std.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a68d30",
   "metadata": {},
   "source": [
    "### Training and evaluation phases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf12843",
   "metadata": {},
   "source": [
    "Since this is a classification task, we will make use of Logistic Regression.\n",
    "\n",
    "Declare and train a Logistic Regression Classifier on your training set. For this, you can use the constructor `LogisticRegression` from sklearn. Check out further information in https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "Choose max_iter=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8348ff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = ...\n",
    "# train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efd9946",
   "metadata": {},
   "source": [
    "Print the predicted probabilities obtained for the test set. You can use `.predict_proba()` for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522811d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_proba = ...\n",
    "prediction_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953969f1",
   "metadata": {},
   "source": [
    "Logistic Regression returns probabilities as predictions, so in order to arrive at a binary prediction, you need to put a threshold on the predicted probabilities. \n",
    "\n",
    "The function below computes a confusion matrix given the true labels, the prediction probabilities, and the chosen decision threshold. Complete the function by completing the formulas to calculate the true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "After this, we will print the confusion matrix for a decision threshold of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bb8e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(true_label, prediction_proba, decision_threshold): \n",
    "    \n",
    "    # Get the predicted label based on the threshold chosen\n",
    "    predict_label = (prediction_proba[:,1]>decision_threshold).astype(int)   \n",
    "                                                                                                                       \n",
    "    TP = np.sum(np.logical_and(predict_label==1, true_label==1))\n",
    "    TN = np.sum(np.logical_and(predict_label==0, true_label==0))\n",
    "    FP = np.sum(np.logical_and(predict_label==1, true_label==0))\n",
    "    FN = np.sum(np.logical_and(predict_label==0, true_label==1))\n",
    "    \n",
    "    confusion_matrix = np.asarray([[TP, FP],\n",
    "                                    [FN, TN]])\n",
    "    return confusion_matrix\n",
    "\n",
    "\n",
    "confusion_matrix_05 = compute_confusion_matrix(test_label, prediction_proba, 0.5)\n",
    "confusion_matrix_05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429ee65b",
   "metadata": {},
   "source": [
    "Let's plot the confusion matrix (code complete):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf99f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(confusion_matrix):\n",
    "    [[TP, FP],[FN, TN]] = confusion_matrix\n",
    "    label = np.asarray([['TP {}'.format(TP), 'FP {}'.format(FP)],\n",
    "                        ['FN {}'.format(FN), 'TN {}'.format(TN)]])\n",
    "    \n",
    "    df_cm = pd.DataFrame(confusion_matrix, index=['Yes', 'No'], columns=['Positive', 'Negative']) \n",
    "    \n",
    "    return sn.heatmap(df_cm, cmap='YlOrRd', annot=label, annot_kws={\"size\": 16}, cbar=False, fmt='')\n",
    "\n",
    "plt.figure(figsize = (6,4)) \n",
    "ax = plot_confusion_matrix(confusion_matrix_05)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Confusion matrix for a 0.5 threshold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee4df41",
   "metadata": {},
   "source": [
    "The function below computes the accuracy, precision, recall, and F1-score with respect to the positive and the negative class. Complete the function by completing the formulas for all these metrics\n",
    "\n",
    "After this, we will print all the scores for a decision threshold of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd37d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_score(confusion_matrix, t=0.5):\n",
    "    [[TP, FP],[FN, TN]] = confusion_matrix.astype(float)\n",
    "    \n",
    "    accuracy = ...\n",
    "    \n",
    "    precision_positive = ...\n",
    "    precision_negative = ...\n",
    "    \n",
    "    recall_positive = ...\n",
    "    recall_negative = ...\n",
    "\n",
    "    F1_score_positive = ...\n",
    "    F1_score_negative = ...\n",
    "\n",
    "    return [t, accuracy, precision_positive, recall_positive, F1_score_positive, precision_negative, recall_negative, F1_score_negative]\n",
    "\n",
    "\n",
    "[t, accuracy, precision_positive, recall_positive, F1_score_positive, \\\n",
    "    precision_negative, recall_negative, F1_score_negative] = compute_all_score(confusion_matrix_05)\n",
    "\n",
    "print(\"The accuracy of this model is {0:1.3f}\".format(accuracy))\n",
    "print(\"For the positive case, the precision is {0:1.3f}, the recall is {1:1.3f} and the F1 score is {2:1.3f}\"\\\n",
    "      .format(precision_positive, recall_positive, F1_score_positive))\n",
    "print(\"For the negative case, the precision is {0:1.3f}, the recall is {1:1.3f} and the F1 score is {2:1.3f}\"\\\n",
    "      .format(precision_negative, recall_negative, F1_score_negative))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a57fc5c",
   "metadata": {},
   "source": [
    "### Further visual analysis (code complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be2ed50",
   "metadata": {},
   "source": [
    "We will vary the value of the threshold in the range from 0 to 1 and visualize the value of accuracy, precision, recall, and F1-score (with respect to both classes) as a function of the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59d43d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.linspace(0, 1, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66cd5df",
   "metadata": {},
   "source": [
    "The code below computes all the metrics for each of the threshold levels, and stores them into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8492e294",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_score_name = ['Threshold', 'Accuracy', 'Precision P', 'Recall P', 'F1 score P', \\\n",
    "                                              'Precision N', 'Recall N', 'F1 score N']\n",
    "threshold_score = pd.concat([pd.DataFrame([compute_all_score(compute_confusion_matrix(test_label, prediction_proba, t ),t)]\\\n",
    "                                             , columns=columns_score_name) for t in threshold], ignore_index=True)\n",
    "threshold_score.set_index('Threshold', inplace=True)\n",
    "\n",
    "threshold_score.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5015cc",
   "metadata": {},
   "source": [
    "We will now plot the accuracy as a function of the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca915428",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_score['Accuracy'].plot(grid=True).set_title('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040fba18",
   "metadata": {},
   "source": [
    "We will now plot the rest of the metrics as a function of the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2883f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=3, sharex=True, sharey=True, figsize=(15,7))\n",
    "\n",
    "col_plot = ['Precision P', 'Recall P', 'F1 score P', 'Precision N', 'Recall N', 'F1 score N']\n",
    "\n",
    "major_ticks = np.linspace(0,1,5)\n",
    "\n",
    "for axe, col in zip(axs.flat, col_plot):\n",
    "    threshold_score[col].plot(ax=axe, grid = True)\n",
    "    axe.set_title(col)\n",
    "    axe.set_xticks(major_ticks)    \n",
    "    axe.grid(which='major', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b94d15e",
   "metadata": {},
   "source": [
    "What do you observe? What do you think a good value for the threshold might be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80443eac",
   "metadata": {},
   "source": [
    "### Feature analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1c84e7",
   "metadata": {},
   "source": [
    "Based on the Logistic Regression model trained, obtain the coefficients associated to each of the features. Check out the `coef_` attribute.\n",
    "\n",
    "**Important**: the array must have 1 dimension only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d64554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_coefficients = logistic.coef_[0]\n",
    "logistic_coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1654d8",
   "metadata": {},
   "source": [
    "We will create an array with the name of the features and the coefficient associated to it (code complete):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17331390",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for name, value in zip(train_features_std.columns, logistic_coefficients):\n",
    "    tmp.append({\"name\": name, \"value\": value})\n",
    "    \n",
    "features_coef = pd.DataFrame(tmp)\n",
    "features_coef.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfe50a2",
   "metadata": {},
   "source": [
    "Sort this DataFrame in ascending order by value. `sort_values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb728ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_coef = features_coef.sort_values(by=['value'])\n",
    "features_coef.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601a2b0f",
   "metadata": {},
   "source": [
    "Let's plot in a bar chart the coefficients of the logistic regression sorted by their contribution to the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe29f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(5,7))\n",
    "plt.barh(features_coef.name, features_coef.value, alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d9f203",
   "metadata": {},
   "source": [
    "How can you interpret this information? **Hint**: recall that\n",
    "$$P(y=1|x,\\beta) =1/(1+\\exp(-\\beta^Tx)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79d968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert your thoughts here#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

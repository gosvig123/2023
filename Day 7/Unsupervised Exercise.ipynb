{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de40e7da-0429-4149-a927-e1e65c30c63c",
   "metadata": {},
   "source": [
    "*Credits: Applied Data Analysis (ADA) course at EPFL (https://dlab.epfl.ch/teaching/fall2020/cs401/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a800233-023f-4c34-bd1d-b4b6f1694250",
   "metadata": {},
   "source": [
    "## Unsupervised Learning exercise\n",
    "\n",
    "Now that we have learned about clustering and dimensionality reduction techniques, let's apply those into a real world example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60da0136-7b6c-424f-b219-d2418e05fb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708210c8-6233-4ca9-9523-d905654a27ce",
   "metadata": {},
   "source": [
    "<img src=\"img/wheat_banner.png\" width=\"800\">\n",
    "\n",
    "Data Set Information:\n",
    "\n",
    "> The examined group comprised kernels belonging to three different varieties of wheat: Kama, Rosa and Canadian, 70 elements each, randomly selected for\n",
    "the experiment. High quality visualization of the internal kernel structure was detected using a soft X-ray technique. It is non-destructive and considerably cheaper than other more sophisticated imaging techniques like scanning microscopy or laser technology. The images were recorded on 13x18 cm X-ray KODAK plates. Studies were conducted using combine harvested wheat grain originating from experimental fields, explored at the Institute of Agrophysics of the Polish Academy of Sciences in Lublin.\n",
    "\n",
    "Source: https://archive.ics.uci.edu/ml/datasets/seeds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a4d585-1798-4e8e-8b0a-b497e68c1838",
   "metadata": {},
   "source": [
    "First, load the data _seeds\\_dataset.csv_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159b67cf-dce5-41c4-a785-69c1f8a1eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = ...\n",
    "seeds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cd1f71-8daa-4d95-8987-aaec5ac47d5e",
   "metadata": {},
   "source": [
    "In this exercise, we are interested in discovering structure in the data by hiding the actual label provided in the dataset.\n",
    "\n",
    "### Prepare the dataset\n",
    "\n",
    "We will create a dataset by keeping only the meaningful features. As a result, remove the _ID_ and _seedType_ columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892d5108-57f4-4d1b-a71b-bc705e869912",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds_features = ...\n",
    "seeds_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fe2f8d-03fe-47c0-9098-72c1cf6b4b12",
   "metadata": {},
   "source": [
    "Plot the histogram of the different features. We will do this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2e2ca7-4256-4056-a551-37b620dcb0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = seeds_features.columns\n",
    "\n",
    "fig, axs = plt.subplots(1, len(columns), figsize=(14,2), sharex=True)\n",
    "\n",
    "for column_idx in range(0, len(columns)):\n",
    "    seeds_features[columns[column_idx]].hist(bins=20, ax=axs[column_idx], alpha=0.6)\n",
    "    axs[column_idx].set_title(columns[column_idx])\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5c4fcb-9b5a-42e3-a5f1-d1bef952c383",
   "metadata": {},
   "source": [
    "Do you notice anything in particular regarding the values of the different features?\n",
    "\n",
    "**Solution below**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9202be6-a73a-44cb-af24-37d633132cbf",
   "metadata": {},
   "source": [
    "The features have different value scales, and this is not ideal since it can lead to them having different importance in the algorithm.\n",
    "\n",
    "For example let's say you are measuring distance between data points with features $u$ (ranging [0, 1]) and $t$ (ranging [0, 1000]). We define the points $p1=(0, 500)$, $p2=(1, 500)$ and $p3=(1, 510)$. If not properly scaled, point $p2$ will be closer to $p1$ than to $p3$, while relatively speaking the latter should be more similar.\n",
    "\n",
    "We will mitigate this problem, by normalizing the data. There are different ways to normalize a dataset, but in this case we will use the `StandardScaler` function implemented by `sklearn`. This function standardizes features by removing the mean and scaling to unit variance. Check out more in the documentation: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "\n",
    "Declare a StandardScaler and fit it with our data features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe49b4c-1d2d-4b92-9b77-102c6f7e38a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Declare the scaler\n",
    "scaler = ...\n",
    "# Fit it\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e87e030-3323-47b2-a4ed-eff0442988bf",
   "metadata": {},
   "source": [
    "Scale the features. You can use the `.transform()` function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb446d1-f07d-4454-8550-131183d1ca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "scaled_features = ...\n",
    "\n",
    "print(\"Scaled sample: %s\" %scaled_features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd20d7d0-f872-42dd-8844-d653eed96271",
   "metadata": {},
   "source": [
    "### Cluster the data with K-Means\n",
    "\n",
    "Firstly, we need to find the most suitable value of K. For this, we will use the Elbow method.\n",
    "\n",
    "The function below computes the SSE for different values of K, and plots its distribution. Complete ir by declaring a K-Means model for each value, training it using the training data, and extracting the SSE.\n",
    "\n",
    "*Hint:* the kmeans object has an attribute containing the Sum of Squared Errors. Check again the documentation (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) to find this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f04ecc-75df-4059-8637-840e5d3ad577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sse(features_X, start=1, end=9):\n",
    "    sse_array = []\n",
    "    for k in range(start, end):\n",
    "        '''\n",
    "        FILL IN YOUR CODE HERE\n",
    "        '''\n",
    "        # Declare the K-Means model. Choose random_state=10\n",
    "        kmeans = ...\n",
    "        # Train it\n",
    "        ...\n",
    "        # Extract the Sum of Squared Errors\n",
    "        sse = ...\n",
    "        \n",
    "        sse_array.append({\"k\": k, \"sse\": sse})\n",
    "\n",
    "    sse_array = pd.DataFrame(sse_array)\n",
    "    # Plot the data\n",
    "    plt.plot(sse_array.k, sse_array.sse)\n",
    "    plt.xlabel(\"K\")\n",
    "    plt.ylabel(\"Sum of Squared Errors\")\n",
    "    \n",
    "plot_sse(scaled_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebe90ee-abbe-43c8-a1e6-876762d69b6b",
   "metadata": {},
   "source": [
    "Based on your findings, make a guess of what should be the appropriate value for `K`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced84975-b82c-49e1-8cb5-ff4847d425e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_k = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94765a4-251e-4c31-8e2a-1f898270c2ce",
   "metadata": {},
   "source": [
    "### Visualize your clusters\n",
    "\n",
    "First, print the number of features considered for this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ac2189-0c8d-4e1e-9120-619819566c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c0ba06-6f1a-4093-a338-1e25b92bd542",
   "metadata": {},
   "source": [
    "Visualizing data with these dimensions is not possible. We need a dimensionality reduction technique to be able to plot this in 2d.\n",
    "\n",
    "Let's make use of [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). Therefore a t-SNE model with a 2-dimensional embedded space. For this, use sklearn's built-in function TSNE. Set random_state=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddcf933-bf39-49d2-a66a-91ed929ec124",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bc578e-bc06-4ba5-8512-13a1d8c767b4",
   "metadata": {},
   "source": [
    "Now, extract the embedded features. For this, you can make use of the `.fit_transform()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dea42d-831d-479e-bdca-b8ffef859a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced_tsne = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36aabe0-0186-487b-a22c-877c37a54873",
   "metadata": {},
   "source": [
    "Declare a K-Means model with the optimal number of clusters you have found. Set random_state=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e600f503-e50a-4d6a-854e-5292d4aaff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4819664f-663b-4d09-9ff9-014943c461c6",
   "metadata": {},
   "source": [
    "Fit the model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7c7de8-2e1f-40f8-8504-de7a6df3d018",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2043340-8b66-4fed-9382-6e185a3e4050",
   "metadata": {},
   "source": [
    "Extract the labels from each datapoint. *Hint*: check out the `.predict()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc211b4-9366-4b3e-bdc9-958f94afab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112eae5a-148f-4761-a4d6-8bd6d313e0ee",
   "metadata": {},
   "source": [
    "Let's plot the resulting clusters, together with the original seed types!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d47b47-77b5-44e8-a7af-a6da0d98da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12,5), sharey=True)\n",
    "\n",
    "# Plot the original seed types\n",
    "axs[0].scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=seeds.seedType, alpha=0.6)\n",
    "axs[0].set_title(\"Original Types\")\n",
    "\n",
    "# Plot the clusters discovered based on the structure of the data\n",
    "axs[1].scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=labels, alpha=0.6)\n",
    "axs[1].set_title(\"Discovered clusters\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

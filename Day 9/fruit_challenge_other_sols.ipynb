{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport piplite\nawait piplite.install('seaborn')\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier",
      "metadata": {
        "trusted": true
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Fruit classification challenge!",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Data**:\n<br>\nWe provide a training dataset comprising 10’000 photoplethysmography (PPG) signals sampled\nat 10 Hz (30-second duration each). Each of these 10’000 PPG recordings has a label\n(fruit name) that can be used to train an ML model.\n\n**Goal**:<br>\nThe idea is to :\n1. Design an ML-based solution to classify these PPG signals into the different fruits and\ntrain it using the training dataset.\n2. Generate the outputs for the 10’000 recordings of the test dataset and upload the results in the shared drive by 12:30h.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Recall common steps to tackle a Machine Learning challenge:\n1. Exploratory Data Analysis\n    -  Check the type of data we have\n    -  Check data and label distribution\n    -  Visualizations (scatterplots, histograms, boxplots, ...)\n2. Feature selection:\n    - Assess which features to use from the data\n    - Iterative process, usually trial-error\n3. Model selection\n    - Try different models and evaluate them via train/validation split or k-fold cross validation\n    - Select best candidate or few best candidates\n4. Model evaluation\n    - Assess the performance of the best model on a test dataset/submit the results on the test dataset\n    ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Let's load the data, make sure to check you are correctly handling the headers of the datasets!",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "data_raw = pd.read_csv('quiz_train_data.csv',header=None)\nfruits_raw = pd.read_csv('quiz_train_labels.csv', header=None)\n\ndata = data_raw.values\nfruits = fruits_raw.values.flatten()\n\nfruit_names = np.unique(fruits)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# functions to for feature generation\ndef normalizer_scaling(X):\n    X = np.array(X)\n    numerator = X - X.min(axis=1).reshape((len(X), 1))\n    denominator = X.max(axis=1) - X.min(axis=1)\n    denominator = denominator.reshape((len(X),1))\n    X_scaled = numerator/denominator\n    return X_scaled\n\n# fft transformation\ndef generate_fft(X):\n    n = X.shape[1]\n    return np.abs(np.fft.fft(X))[:,1:n//2], np.arange(1, (n//2))/n\n\ndef generate_features(X):\n    # Compute mean and std of raw signal\n    X_mean = np.mean(X,axis=1)\n    X_std = np.std(X,axis=1)\n    X_min = np.min(X,axis=1)\n    X_max = np.max(X,axis=1)\n    \n    \n    # Normalize signal and retrieve fft\n    X_scaled = normalizer_scaling(X)\n    X_fft, _ = generate_fft(X_scaled)\n    return np.column_stack((X_mean,X_std,X_min,X_max,X))\n\ndef report_accuracy(X, y, model, model_name):\n    predictions = model.predict(X)\n    accuracy = np.mean(predictions==y)\n    print(\"%s Validation accuracy is %.3f\" % (model_name, accuracy))\n    ",
      "metadata": {
        "trusted": true
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Train-validation split ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "X = generate_features(data)\ny = fruits",
      "metadata": {
        "trusted": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "label_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=40)\nprint(X_train.shape, X_val.shape)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "text": "(8000, 304) (2000, 304)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "# Model training",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Logistic regression\nmodel_lr = LogisticRegression(max_iter=100).fit(X_train, y_train)\n# MLP\nmodel_mlp = MLPClassifier(hidden_layer_sizes=(20,20,20), max_iter=1000).fit(X_train, y_train)\n# XGBoost\nmodel_xgb = XGBClassifier(n_estimators=20).fit(X_train, y_train,verbose=True)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 36,
      "outputs": [
        {
          "name": "stderr",
          "text": "/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "report_accuracy(X_val, y_val, model_lr, 'Logistic Regression')\nreport_accuracy(X_val, y_val, model_mlp, 'MLP')\nreport_accuracy(X_val, y_val, model_xgb, 'XGBoost')",
      "metadata": {
        "trusted": true
      },
      "execution_count": 37,
      "outputs": [
        {
          "name": "stdout",
          "text": "Logistic Regression Validation accuracy is 0.296\nMLP Validation accuracy is 0.536\nXGBoost Validation accuracy is 0.629\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "# Evaluate on the test set",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "test_data_raw = pd.read_csv('quiz_test_data.csv',header=None)\ntest_labels = pd.read_csv('quiz_test_labels.csv',header=None).values.flatten()\ntest_data = test_data_raw.values",
      "metadata": {
        "trusted": true
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "for model in [model_lr, model_mlp, model_xgb]:\n    test_predictions = model.predict(X_test)\n    test_fruit_predictions = label_encoder.inverse_transform(test_predictions)\n    print(np.mean(test_fruit_predictions==test_labels))",
      "metadata": {
        "trusted": true
      },
      "execution_count": 39,
      "outputs": [
        {
          "name": "stdout",
          "text": "0.2928\n0.5386\n0.6332\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}
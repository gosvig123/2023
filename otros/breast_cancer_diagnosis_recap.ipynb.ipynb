{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ae0f114b-aa26-5802-345f-83fd12c570fe"
   },
   "source": [
    "## Machine Learning Recap: Classifying Breast Cancer Using ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bbb53629-d2a7-ffc6-8e0c-9f3cf2f9c497"
   },
   "source": [
    "Welcome to this recap of our week-long intensive machine learning course! In this interactive notebook, we will focus on a critical application of machine learning â€“ classifying breast cancer.\n",
    "\n",
    "Breast cancer is a significant health concern, and accurate diagnosis is crucial for effective treatment. By using basic machine learning algorithms, we can contribute to this important field and showcase the practicality of machine learning in real-life scenarios.\n",
    "\n",
    "Our dataset is the well-known Breast Cancer Wisconsin (Diagnostic) dataset, which provides information on 30 different characteristics of cell nuclei. We will use these features to predict the stage of breast cancer, classifying it as either malignant (M) or benign (B).\n",
    "\n",
    "Throughout this notebook, we will explore fundamental machine learning algorithms, step by step, with clear explanations and easy-to-follow implementations. Our goal is to make this recap accessible and enjoyable for beginners.\n",
    "\n",
    "Before we dive into the models, let's understand the attribute information in the dataset. It includes an ID number, diagnosis (malignant or benign), and ten real-valued features for each cell nucleus. These features capture important characteristics like radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, and fractal dimension.\n",
    "\n",
    "The features are categorized as Mean, Standard Error, and Worst, each containing ten parameters. Mean represents the average values, Standard Error indicates the measurement's variability, and Worst represents the most concerning cell characteristics.\n",
    "\n",
    "Get ready to embark on this exciting journey where we combine the power of machine learning with the vital task of breast cancer classification. Let's dive in and explore the models together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e3200e6e-62d4-1d74-496b-c5197a574dcb"
   },
   "outputs": [],
   "source": [
    "# here we will import the libraries used for machine learning\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL\n",
    "import matplotlib.pyplot as plt # this is used for the plot the graph \n",
    "# import seaborn as sns # used for plot interactive graph.\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LogisticRegression # to apply the Logistic regression\n",
    "from sklearn.model_selection import train_test_split # to split the data into two parts\n",
    "# from sklearn.cross_validation import KFold # use for cross validation\n",
    "from sklearn.model_selection import GridSearchCV# for tuning parameter\n",
    "from sklearn.ensemble import RandomForestClassifier # for random forest classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm # for Support Vector Machine\n",
    "from sklearn import metrics # for the check the error and accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fa78428b-ecbb-56a4-2904-f90d725281ec"
   },
   "source": [
    "Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7036d299-0057-db0a-d892-fe212c0d612c"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/data.csv\",header=0)# here header 0 means the 0 th row is our coloumn \n",
    "                                                # header in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3521dc85-be7e-a735-0cc1-b3437c16ab22"
   },
   "outputs": [],
   "source": [
    "# have a look at the data\n",
    "print(data.head(2))# as u can see our data have imported and having 33 columns\n",
    "# head is used for to see top 5 by default I used 2 so it will print 2 rows\n",
    "# If we will use print(data.tail(2))# it will print last 2 rows in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1109ea03-5b73-25fa-72ca-6695de4b0c5b"
   },
   "outputs": [],
   "source": [
    "# now lets look at the type of data we have. We can use \n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9041739c-1d90-970b-55ec-224ed821898a"
   },
   "source": [
    "1. So lets describe what these data type means, e.g 5 radius_mean 569 non-null float64 that means the radius_mean have 569 float type value.\n",
    "\n",
    "2. Now we can see Unnamed:32 have 0 non null object it means the all values are null in this column so we cannot use this column for our analysis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9ec57be7-c20b-5895-2f20-0f32651b3f79"
   },
   "outputs": [],
   "source": [
    "# now we can drop this column Unnamed: 32\n",
    "data.drop(\"Unnamed: 32\",axis=1,inplace=True) # in this process this will change in our data itself \n",
    "# here axis 1 means we are droping the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a93a7754-8b38-6ca4-cc7d-e31cfd58afa6"
   },
   "outputs": [],
   "source": [
    "# here you can check the column has been droped\n",
    "data.columns # this gives the column name which are persent in our data no Unnamed: 32 is not now there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f0da592b-04ff-9c69-b5b0-46e87ebc391d"
   },
   "outputs": [],
   "source": [
    "# like this we also don't want the Id column for our analysis\n",
    "data.drop(\"id\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "68f07b98-c05a-f3e6-7dbe-8e39ab7d13f6"
   },
   "outputs": [],
   "source": [
    "# As I said above the data can be divided into three parts.lets divied the features according to their category\n",
    "features_mean= list(data.columns[1:11])\n",
    "features_se= list(data.columns[11:20])\n",
    "features_worst=list(data.columns[21:31])\n",
    "print(features_mean)\n",
    "print(\"-----------------------------------\")\n",
    "print(features_se)\n",
    "print(\"------------------------------------\")\n",
    "print(features_worst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dbdd0bf6-da03-c390-0e7a-f6b181f8e127"
   },
   "outputs": [],
   "source": [
    "# lets now start with features_mean \n",
    "# now as ou know our diagnosis column is a object type so we can map it to integer value\n",
    "data['diagnosis']=data['diagnosis'].map({'M':1,'B':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "19034b94-8ddf-ef3f-675d-c87b6a558c1b"
   },
   "source": [
    "## Explore the Data now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "09c22a48-dbc2-f1c3-517a-491564ab161e"
   },
   "outputs": [],
   "source": [
    "data.describe() # this will describe the all statistical function of our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5b9a0c99-86ca-9b5e-687d-8da91bcf9ff5"
   },
   "source": [
    "## Data Analysis a little feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "592f4ece-d0b1-cccf-972e-3b7feace219b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_subset = data[['diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n",
    "                   'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean',\n",
    "                   'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se',\n",
    "                   'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se',\n",
    "                   'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst',\n",
    "                   'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst',\n",
    "                   'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst']]\n",
    "\n",
    "cor = data_subset.corr()  # Calculate the correlation of the variables\n",
    "\n",
    "# Create a figure and axes\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Define the heatmap properties\n",
    "heatmap = ax.imshow(cor, cmap='coolwarm')\n",
    "\n",
    "# Set the tick labels and font size\n",
    "ax.set_xticks(range(len(cor.columns)))\n",
    "ax.set_yticks(range(len(cor.columns)))\n",
    "ax.set_xticklabels(cor.columns, rotation=45, ha='right', fontsize=10)\n",
    "ax.set_yticklabels(cor.columns, rotation=0, ha='right', fontsize=10)\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_xlabel('Features', fontsize=12)\n",
    "ax.set_ylabel('Features', fontsize=12)\n",
    "\n",
    "# Set the title\n",
    "ax.set_title('Correlation Heatmap', fontsize=14)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(heatmap)\n",
    "\n",
    "# Remove the gridlines\n",
    "ax.grid(False)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b79fcdb1-8104-291f-64a1-2b5faa145f0d"
   },
   "source": [
    "### Observations:\n",
    "\n",
    "- We observe a strong correlation among the features radius, perimeter, and area, which is expected due to their inherent relationship. Therefore, we can choose any one of these features for our analysis.\n",
    "\n",
    "- Among the features compactness_mean, concavity_mean, and concave points_mean, there is a significant correlation. In this case, we will select compactness_mean as our representative feature.\n",
    "\n",
    "Based on these observations, the selected parameters for use in our analysis are:\n",
    "- Perimeter_mean\n",
    "- Texture_mean\n",
    "- Compactness_mean\n",
    "- Symmetry_mean\n",
    "\n",
    "These features exhibit distinct correlations and will be valuable in our classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fcd92fe8-fb89-d377-f135-c053e562e820"
   },
   "outputs": [],
   "source": [
    "prediction_var = ['texture_mean','perimeter_mean','smoothness_mean','compactness_mean','symmetry_mean']\n",
    "# now these are the variables which will use for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "da2eaf5c-6f86-b320-a591-fb7b7e0fb12c"
   },
   "outputs": [],
   "source": [
    "#now split our data into train and test\n",
    "train, test = train_test_split(data, test_size = 0.3)# in this our main data is splitted into train and test\n",
    "# we can check their dimension\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "27cb4563-3191-8ab9-8ce2-224c2db223d0"
   },
   "outputs": [],
   "source": [
    "train_X = train[prediction_var]# taking the training data input \n",
    "train_y=train.diagnosis# This is output of our training data\n",
    "# same we have to do for test\n",
    "test_X= test[prediction_var] # taking test data inputs\n",
    "test_y =test.diagnosis   #output value of test dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4936a3a9-e7fd-81a1-1aec-a267224569d5"
   },
   "outputs": [],
   "source": [
    "model=RandomForestClassifier(n_estimators=100)# a simple random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c20ee887-14c4-fd6c-98fe-d54c06d6d4e4"
   },
   "outputs": [],
   "source": [
    "model.fit(train_X,train_y)# now fit our model for traiing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "abb767bd-ed3e-6af0-9aa5-55ccbf50f9c7"
   },
   "outputs": [],
   "source": [
    "prediction=model.predict(test_X)# predict for the test data\n",
    "# prediction will contain the predicted value by our model predicted values of dignosis column for test inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f37e4a4e-214c-9f56-391b-19a9a6b141ec"
   },
   "outputs": [],
   "source": [
    "metrics.accuracy_score(prediction,test_y) # to check the accuracy\n",
    "# here we will use accuracy measurement between our predicted value and our test output values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b8ae477d-f309-03b1-e224-f993ee9fed5b"
   },
   "source": [
    "* Here the Accuracy for our model is 91 % which seems good*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now try with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "74acfdf3-a6c1-e262-eaf5-742c62bf1382"
   },
   "outputs": [],
   "source": [
    "model = svm.SVC()\n",
    "model.fit(train_X,train_y)\n",
    "prediction=model.predict(test_X)\n",
    "metrics.accuracy_score(prediction,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3a2a515c-5b1f-4086-48cb-96dbaa67815c"
   },
   "source": [
    "**SVM is giving only 0.85 which we can improve by using different techniques** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "87950903-844c-058d-23d5-2d55ae58de75"
   },
   "source": [
    "*Now lets do this for all feature_mean so that from Random forest we can get the feature which are important**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4dd23ad4-a0e5-09e3-4319-187051f24199"
   },
   "outputs": [],
   "source": [
    "prediction_var = features_mean # taking all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8ffa199c-9f4e-8574-430d-d3133ae0d8c6"
   },
   "outputs": [],
   "source": [
    "train_X= train[prediction_var]\n",
    "train_y= train.diagnosis\n",
    "test_X = test[prediction_var]\n",
    "test_y = test.diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "20b8aaf5-677c-a2af-c998-dfd5fa713115"
   },
   "outputs": [],
   "source": [
    "model=RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b55cdfda-aab0-4130-990f-506754f16734"
   },
   "outputs": [],
   "source": [
    "model.fit(train_X,train_y)\n",
    "prediction = model.predict(test_X)\n",
    "metrics.accuracy_score(prediction,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8d0c1af5-f907-00c5-e189-3dc1e78c032b"
   },
   "source": [
    " - By taking all features accuracy increased but not so much so according to Razor's rule simpler method is better\n",
    " - By the way now lets check the important features in the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "de3e85a8-20c3-8ebf-57f0-68dd360e0816"
   },
   "outputs": [],
   "source": [
    "featimp = pd.Series(model.feature_importances_, index=prediction_var).sort_values(ascending=False)\n",
    "print(featimp) # this is the property of Random Forest classifier that it provide us the importance \n",
    "# of the features used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets do with SVM also using all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c0ae8f9c-05a7-e4b4-c7ac-4ca46c69c66e"
   },
   "outputs": [],
   "source": [
    "model = svm.SVC()\n",
    "model.fit(train_X,train_y)\n",
    "prediction=model.predict(test_X)\n",
    "metrics.accuracy_score(prediction,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed, the accuracy of the SVM significantly decreases. Therefore, let's proceed by considering only the top 5 important features identified by the RandomForest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "36a11667-fce7-4fa0-8b84-7780230ef339"
   },
   "outputs": [],
   "source": [
    "prediction_var=['concave points_mean','perimeter_mean' , 'concavity_mean' , 'radius_mean','area_mean']      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f9aafdb3-1ee7-56d7-57d0-ac009f9a9522"
   },
   "outputs": [],
   "source": [
    "train_X= train[prediction_var]\n",
    "train_y= train.diagnosis\n",
    "test_X = test[prediction_var]\n",
    "test_y = test.diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "37caf223-82ef-0579-32c8-de2a530faa3a"
   },
   "outputs": [],
   "source": [
    "model=RandomForestClassifier(n_estimators=100)\n",
    "model.fit(train_X,train_y)\n",
    "prediction = model.predict(test_X)\n",
    "metrics.accuracy_score(prediction,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "113982ab-8065-3e27-92b8-0716924138e7"
   },
   "outputs": [],
   "source": [
    "model = svm.SVC()\n",
    "model.fit(train_X,train_y)\n",
    "prediction=model.predict(test_X)\n",
    "metrics.accuracy_score(prediction,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this discussion, it becomes apparent that multicollinearity has a substantial impact on the SVM model, while it doesn't affect the Random Forest model to the same extent. This highlights the difference in effort required for analysis between the two models. Moving forward, let's focus on the third part of the data, which pertains to the \"worst\" features. We will begin by considering all the features in the \"worst\" category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "33c5c58a-31af-6309-1c6c-dd2280c0b112"
   },
   "outputs": [],
   "source": [
    "prediction_var = features_worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e5a92132-7ec3-8901-e52e-b42973ae34f6"
   },
   "outputs": [],
   "source": [
    "train_X= train[prediction_var]\n",
    "train_y= train.diagnosis\n",
    "test_X = test[prediction_var]\n",
    "test_y = test.diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3f2fb8a5-176f-588f-86b9-676b4d94493c"
   },
   "outputs": [],
   "source": [
    "model = svm.SVC()\n",
    "model.fit(train_X,train_y)\n",
    "prediction=model.predict(test_X)\n",
    "metrics.accuracy_score(prediction,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1e0aa84e-24ea-496a-81ac-ddc145e2c3cd"
   },
   "outputs": [],
   "source": [
    "# but same problem With SVM, very much less accuray I think we have to tune its parameter\n",
    "# that i will do later in intermidate part\n",
    "#now we can get the important features from random forest now run Random Forest for it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "942c21fa-86f8-eb02-3dcc-e71c3bdaa1c6"
   },
   "outputs": [],
   "source": [
    "model=RandomForestClassifier(n_estimators=100)\n",
    "model.fit(train_X,train_y)\n",
    "prediction = model.predict(test_X)\n",
    "metrics.accuracy_score(prediction,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6fdbaa7b-2d17-d7df-91ac-d0b2d40ee29d"
   },
   "outputs": [],
   "source": [
    "# the accuracy for RandomForest invcrease it means the value are more catogrical in Worst part\n",
    "#lets get the important features\n",
    "featimp = pd.Series(model.feature_importances_, index=prediction_var).sort_values(ascending=False)\n",
    "print(featimp) # this is the property of Random Forest classifier that it provide us the importance \n",
    "# of the features used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b123094f-0ec6-a780-d247-e455e814b475"
   },
   "outputs": [],
   "source": [
    "# same parameter but with great importance and here it seamed the only conacve points_worst is making \n",
    "# very important so it may be bias lets check only for top 5 important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "152b0a6f-b34b-adc4-8c71-f9e46143e635"
   },
   "outputs": [],
   "source": [
    "prediction_var = ['concave points_worst','radius_worst','area_worst','perimeter_worst','concavity_worst'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c0157475-6e2b-0462-9898-5a443afb9d8c"
   },
   "outputs": [],
   "source": [
    "train_X= train[prediction_var]\n",
    "train_y= train.diagnosis\n",
    "test_X = test[prediction_var]\n",
    "test_y = test.diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0e9d0dcd-f06d-c281-1152-691a99754bc1"
   },
   "outputs": [],
   "source": [
    "model=RandomForestClassifier(n_estimators=100)\n",
    "model.fit(train_X,train_y)\n",
    "prediction = model.predict(test_X)\n",
    "metrics.accuracy_score(prediction,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b06553a2-25f9-ece8-00ef-cf139d887d04"
   },
   "outputs": [],
   "source": [
    "#check for SVM\n",
    "model = svm.SVC()\n",
    "model.fit(train_X,train_y)\n",
    "prediction=model.predict(test_X)\n",
    "metrics.accuracy_score(prediction,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the need for simplicity, it seems that Random Forest would be a more suitable choice for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's further explore the data. We will focus on the features_mean and attempt to identify variables that can be used for classification by plotting a scatter plot. Our objective is to find variables that exhibit a distinct boundary between the two cancer classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin the data analysis by examining the features_mean. Our aim is to determine which features can be used for prediction. I will create scatter plots for all the features_mean, showcasing the data points for both diagnosis categories. Through this visualization, we can identify the features that display a noticeable distinction between the two categories and can be effectively used for differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "834d5543-115d-55f9-0289-5709ed068e7d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "color_function = {0: \"blue\", 1: \"red\"}  # Red color represents 1 (M) and blue represents 0 (B)\n",
    "colors = data[\"diagnosis\"].map(lambda x: color_function.get(x))  # Mapping the color function with the diagnosis column\n",
    "\n",
    "pd.plotting.scatter_matrix(data[features_mean], c=colors, alpha=0.5, figsize=(15, 15))  # Plotting scatter plot matrix\n",
    "\n",
    "plt.show()  # Display the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "78883101-b683-4a38-efdf-961db36f673a"
   },
   "source": [
    "### Observations\n",
    "\n",
    "1. Radius, area and perimeter have a strong linear relationship as expected\n",
    "2. As graph shows the features like as texture_mean, smoothness_mean, symmetry_mean and fractal_dimension_mean can t be used for classify two category because both category are mixed there is no separable plane\n",
    "3. So we can remove them from our prediction_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7fb4459c-b7c2-8dda-ff20-29757ca229fe"
   },
   "outputs": [],
   "source": [
    "# So predicton features will be \n",
    "features_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8a799df8-71c3-590a-f05e-2bbb4e8afa99"
   },
   "outputs": [],
   "source": [
    "# So predicton features will be \n",
    "predictor_var = ['radius_mean','perimeter_mean','area_mean','compactness_mean','concave points_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8ab6c00c-e045-3ed7-d8fa-cb59629ed18a"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def model(model, data, prediction, outcome):\n",
    "    # This function will be used to check the accuracy of different models\n",
    "    kf = KFold(n_splits=10) # Define the number of folds for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fa211445-a1b2-9521-3502-24d39562ddda"
   },
   "outputs": [],
   "source": [
    "prediction_var = ['radius_mean','perimeter_mean','area_mean','compactness_mean','concave points_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features that have the ability to classify the classes will be more valuable in our analysis. In this section, I will provide an overview of some machine learning concepts. Additionally, I will compare the accuracy of different models and demonstrate the use of cross-validation. Furthermore, I will explain the process of tuning model parameters using gridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "def classification_model(model, data, prediction_input, output):\n",
    "    # This function is used to evaluate the accuracy of different models\n",
    "    model.fit(data[prediction_input], data[output])  # Fit the model using the training set\n",
    "\n",
    "    predictions = model.predict(data[prediction_input])  # Make predictions on the training set\n",
    "\n",
    "    accuracy = metrics.accuracy_score(predictions, data[output])  # Calculate accuracy on the same data\n",
    "    print(\"Accuracy: %s\" % \"{0:.3%}\".format(accuracy))\n",
    "\n",
    "    cv_scores = cross_val_score(model, data[prediction_input], data[output], cv=5)  # Perform cross-validation\n",
    "    print(\"Cross-Validation Scores: \", cv_scores)\n",
    "    print(\"Mean Cross Validation Accuracy: %s\" % \"{0:.3%}\".format(np.mean(cv_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a989f695-6939-743c-c3dd-6f6203ad7f02"
   },
   "source": [
    "Now from here on start using different model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f8038059-da27-77a5-a67a-2a3936149919"
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "prediction_var = ['radius_mean','perimeter_mean','area_mean','compactness_mean','concave points_mean']\n",
    "outcome_var= \"diagnosis\"\n",
    "classification_model(model,data,prediction_var,outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move on to SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "95e14b94-d970-318a-c94f-89bb8d253d78"
   },
   "outputs": [],
   "source": [
    "model = svm.SVC()\n",
    "\n",
    "classification_model(model,data,prediction_var,outcome_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "593073d9-7f25-06f4-2887-c937e0212da8"
   },
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier()\n",
    "classification_model(model,data,prediction_var,outcome_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a0bc621f-8cd3-62aa-0058-79cad0313e94"
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100)\n",
    "classification_model(model,data,prediction_var,outcome_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "be1ef210-09d6-1e8e-1972-4b7b7efda883"
   },
   "outputs": [],
   "source": [
    "model=LogisticRegression()\n",
    "classification_model(model,data,prediction_var,outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "42918503-55a6-b2ac-256a-81b8adfd67ad"
   },
   "source": [
    "### We just saw a detailed comparison of some Machine Learning models \n",
    "\n",
    " 1. In next segment we will see the tuning of parameter for different models\n",
    " 2. Then using those parameter we will try to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a67b38c7-d64d-04e1-7d29-ec2476102e41"
   },
   "source": [
    "### Tuning Parameters  using grid search CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ed127fda-4f0b-5e94-9a9e-c5e56701050b"
   },
   "source": [
    "Lets Start with decision tree classifier:\n",
    "\n",
    "Tuning the parameters means using the best parameter for predict \n",
    " there are many parameters need to model a Machine learning Algorithm\n",
    " for decision tree classifier refer this link [Link](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c9a0f2cf-8ac9-e434-d300-54f898f74d04"
   },
   "outputs": [],
   "source": [
    "data_X= data[prediction_var]\n",
    "data_y= data[\"diagnosis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fff56e83-608f-bee1-8cdf-d75b07cab36a"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def Classification_model_gridsearchCV(model, param_grid, data_X, data_y):\n",
    "    clf = GridSearchCV(model, param_grid, cv=10, scoring=\"accuracy\")\n",
    "    clf.fit(data_X, data_y)\n",
    "    \n",
    "    print(\"The best parameter found on the development set is:\")\n",
    "    print(clf.best_params_)\n",
    "    \n",
    "    print(\"The best estimator is:\")\n",
    "    print(clf.best_estimator_)\n",
    "    \n",
    "    print(\"The best score is:\")\n",
    "    print(clf.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_features': ['sqrt', 'log2'],\n",
    "              'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10], \n",
    "              'min_samples_leaf': [2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
    "model = DecisionTreeClassifier()\n",
    "Classification_model_gridsearchCV(model, param_grid, data_X, data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "67fb10fa-b386-2c7c-6c3c-d23119ab0501"
   },
   "source": [
    "## Observations\n",
    "\n",
    "1. The accuracy score has significantly increased to 95%.\n",
    "2. This is a substantial improvement and indicates the effectiveness of the tuned parameters.\n",
    "3. Next, let's explore the K-Nearest Neighbors (KNN) algorithm.\n",
    "4. For more details on KNN, you can refer to the [Link](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html).\n",
    "5. If you are a beginner, I highly recommend following the provided link as it will provide valuable information and insights on KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f5238051-7292-5a2a-f255-cd68c9797125"
   },
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier()\n",
    "\n",
    "k_range = list(range(1, 30))\n",
    "leaf_size = list(range(1,30))\n",
    "weight_options = ['uniform', 'distance']\n",
    "param_grid = {'n_neighbors': k_range, 'leaf_size': leaf_size, 'weights': weight_options}\n",
    "Classification_model_gridsearchCV(model,param_grid,data_X,data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d7918a00-0d11-f883-98a5-cab998584c03"
   },
   "source": [
    " 1. Try with SVM\n",
    " 2. [link](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0c2e19e8-bd5f-45d3-3134-845c0c4ad11f"
   },
   "outputs": [],
   "source": [
    "model=svm.SVC()\n",
    "param_grid = [\n",
    "              {'C': [1, 10, 100, 1000], \n",
    "               'kernel': ['linear']\n",
    "              },\n",
    "              {'C': [1, 10, 100, 1000], \n",
    "               'gamma': [0.001, 0.0001], \n",
    "               'kernel': ['rbf']\n",
    "              },\n",
    " ]\n",
    "Classification_model_gridsearchCV(model,param_grid,data_X,data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a3e8c0c9-1a3a-047e-0fa3-bdfff182caaa"
   },
   "source": [
    "### Observations\n",
    "\n",
    "1. The SVM model is performing well with the optimal parameters, highlighting the importance of parameter tuning.\n",
    "2. Initially, using the default parameters, the accuracy was only 70%.\n",
    "3. However, after tuning the parameters, the accuracy significantly improved to 95%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e1b37871-b764-9553-b059-108aa553d43b"
   },
   "source": [
    "1. Similarly, we can apply the same approach to the Random Forest classifier.\n",
    "2. However, for the sake of brevity, I will not provide the code for the Random Forest classifier in this context.\n",
    "3. If someone is using this as a reference and wants to explore the Random Forest classifier, I encourage them to apply the same techniques discussed for parameter tuning and evaluation to the Random Forest classifier as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bc71536e-4f9f-0f5c-9363-3f27b643235f"
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "1. The primary goal of this notebook is to offer a comprehensive introduction to various machine learning methods.\n",
    "2. Thank you for your attention and I hope you find this notebook valuable in your machine learning journey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 2,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
